{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# vision_tower_name = 'openai/clip-vit-large-patch14-336' \n","vision_tower_name = 'openai/clip-vit-base-patch32' ## torch.Size([1, 49, 768])\n","image_processor = CLIPImageProcessor.from_pretrained(vision_tower_name)\n","vision_tower = CLIPVisionModel.from_pretrained(vision_tower_name)\n","_ = vision_tower.requires_grad_(False)\n","\n","vision_tower = vision_tower.to(\"cuda:1\")"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def feature_select(image_forward_outs):\n","    image_features = image_forward_outs.hidden_states[-1] # last layer\n","    # print(image_features.shape) # 1, 50, 768\n","    image_features = image_features[:, 1:, :]\n","    return image_features # 1, 49, 768"]},{"cell_type":"markdown","metadata":{},"source":["## Instruct 150 k coco images --> CLIP embedding"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# import pandas as pd\n","# df=pd.read_json(\"/media/App/amaranth/lavanya/Capstone_data/llava_instruct_150k.json\")\n","# df.head()\n","\n","# from datasets import Dataset\n","# dataset = Dataset.from_pandas(df)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# # Using on sample of COCO \n","# import os\n","# from PIL import Image\n","# import h5py\n","\n","# for item in dataset:\n","#     image_path = f\"/media/App/amaranth/lavanya/Capstone_data/train2017/{item['image']}\"\n","#     if os.path.exists(image_path):\n","#         image = Image.open(image_path)\n","#         image = image_processor(images=image, return_tensors=\"pt\")\n","#         image_forward_out = vision_tower(image['pixel_values'].to(device=vision_tower.device), output_hidden_states=True)\n","#         image_feature = feature_select(image_forward_out)\n","#         print(image_feature.shape)\n","#         with h5py.File(f'/media/App/amaranth/lavanya/Capstone_data/clip_features_base_patch32/{os.path.splitext(item[\"image\"])[0]}.h5', 'w') as f:\n","#             f.create_dataset('image_features', data=image_feature.cpu().numpy())"]},{"cell_type":"markdown","metadata":{},"source":["## Coco train2017 full datatset --> CLIP embeddings"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["import glob\n","import os \n","from PIL import Image \n","import h5py\n","train_img_names = glob.glob('/media/App/amaranth/lavanya/Capstone_data/train2017/*.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for image_path in train_img_names: \n","    if os.path.exists(image_path):\n","        image = Image.open(image_path)\n","        image = image_processor(images=image, return_tensors=\"pt\")\n","        image_forward_out = vision_tower(image['pixel_values'].to(device=vision_tower.device), output_hidden_states=True)\n","        image_feature = feature_select(image_forward_out)\n","        with h5py.File(f'/media/App/amaranth/lavanya/Capstone_data/clip_features_base_patch32/{os.path.basename(image_path).split(\".\")[0]}.h5', 'w') as f:\n","            f.create_dataset('image_features', data=image_feature.cpu().numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}

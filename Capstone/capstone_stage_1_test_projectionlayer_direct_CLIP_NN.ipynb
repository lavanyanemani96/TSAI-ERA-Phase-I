{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec8a4bc10bf463599c858aab772ae52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from transformers import  AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "phi2_model_pretrained = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd \n",
    "import json\n",
    "import os \n",
    "import h5py\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1478153/1237926302.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  captions_info_df = pd.read_csv('captions_images_map_COCO_train2017.csv')\n"
     ]
    }
   ],
   "source": [
    "captions_info_df = pd.read_csv('captions_images_map_COCO_train2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCO_CLIP_Dataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, caption_file, embedding_path, tokenizer, max_token_len_data, phi2_model_pretrained, max_seq_len):\n",
    "        \n",
    "        self.embedding_path = embedding_path\n",
    "        self.caption_file = caption_file\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len_data = max_token_len_data\n",
    "        self.phi2_model = phi2_model_pretrained\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.caption_file)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        row = self.caption_file.iloc[[index]]\n",
    "\n",
    "        df_img = row['image_id'].values[0]\n",
    "        img_base_name = '0'*(12-len(str(df_img))) + str(df_img)\n",
    "        img_base_name = img_base_name.replace(' ', '0')\n",
    "        img_clip_embedding_path = os.path.join(self.embedding_path, f'{img_base_name}.h5')\n",
    "\n",
    "        np_array_embed_img = h5py.File(img_clip_embedding_path,'r+')['image_features'][()]\n",
    "        \n",
    "        img_caption = row['caption'].values[0] ## Tokenize this \n",
    "        img_caption_tokenized = self.tokenizer(img_caption, return_tensors=\"pt\", \n",
    "                                               return_attention_mask=False).input_ids\n",
    "        pad_len = self.max_seq_len - img_caption_tokenized.shape[1]\n",
    "        if pad_len != 0: \n",
    "            pad_tokens = torch.tensor([self.tokenizer.eos_token_id]*pad_len).unsqueeze(0)\n",
    "            img_caption_tokenized = torch.cat((img_caption_tokenized, pad_tokens), dim=-1)\n",
    "            \n",
    "        img_caption_embedding = self.phi2_model.get_input_embeddings()(img_caption_tokenized)\n",
    "        \n",
    "        return torch.tensor(np_array_embed_img).squeeze(0), img_caption_embedding.squeeze(0).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exists(image_id, fpath = '/media/App/amaranth/lavanya/Capstone_data/clip_features_base_patch32/'): \n",
    "\n",
    "    n = '0'*(12-len(str(image_id))) + str(image_id) + '.h5'\n",
    "    fp = os.path.join(fpath, n)\n",
    "\n",
    "    if os.path.exists(fp): \n",
    "        return True\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### captions_info_df contains for 1 image multiple entries, lets reduce keeping one image, one entry. \n",
    "captions_info_df_subset = captions_info_df.drop_duplicates(subset='image_id', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token_len_data = 75\n",
    "phi2_embed_dim = 2560\n",
    "clip_embed_patch = 768\n",
    "clip_embed_token= 49 \n",
    "\n",
    "dataset = COCO_CLIP_Dataset(captions_info_df_subset, \n",
    "                            '/media/App/amaranth/lavanya/Capstone_data/clip_features_base_patch32/', \n",
    "                            tokenizer, max_token_len_data, phi2_model_pretrained, max_token_len_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, clip_embed_patch=clip_embed_patch, \n",
    "                 max_seq_len=max_token_len_data, \n",
    "                 phi2_embed_dim=phi2_embed_dim): \n",
    "        \n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.clip_embed_patch = clip_embed_patch\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.phi2_embed_dim = phi2_embed_dim\n",
    "        \n",
    "        # Global Average Pooling layer\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Linear layer\n",
    "        self.fc1 = nn.Linear(self.clip_embed_patch, 3000)\n",
    "        self.fc2 = nn.Linear(3000, 3000)\n",
    "        self.fc3 = nn.Linear(3000, self.max_seq_len*self.phi2_embed_dim)\n",
    "\n",
    "        # Optional activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):        \n",
    "        # Global Average Pooling\n",
    "        x = self.global_avg_pooling(x.transpose(1, 2)).squeeze(dim=2)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        x = x.view(-1, self.max_seq_len, self.phi2_embed_dim)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SimpleResBlock(nn.Module):\n",
    "#     def __init__(self, input_size):\n",
    "#         super().__init__()\n",
    "#         self.pre_norm = nn.LayerNorm(input_size)\n",
    "#         self.proj = nn.Sequential(\n",
    "#             nn.Linear(input_size, input_size),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(input_size, input_size)\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         x = self.pre_norm(x)\n",
    "#         return x + self.proj(x)\n",
    "    \n",
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self, clip_embed_patch=clip_embed_patch, clip_embed_token = clip_embed_token,\n",
    "#                  max_seq_len=max_token_len_data, \n",
    "#                  phi2_embed_dim=phi2_embed_dim): \n",
    "        \n",
    "#         super(MyModel, self).__init__()\n",
    "        \n",
    "#         self.clip_embed_patch = clip_embed_patch\n",
    "#         self.max_seq_len = max_seq_len\n",
    "#         self.phi2_embed_dim = phi2_embed_dim\n",
    "#         self.clip_embed_token = clip_embed_token\n",
    "        \n",
    "#         self.linear_1 = nn.Linear(self.clip_embed_patch, 1500) \n",
    "#         self.linear_2 = nn.Linear(1500, 1500)\n",
    "#         self.linear_3 = nn.Linear(1500, self.phi2_embed_dim)\n",
    "        \n",
    "#         self.projection_1 = SimpleResBlock(self.phi2_embed_dim)   \n",
    "        \n",
    "#         self.fc4 = nn.Linear(self.clip_embed_token, self.max_seq_len)\n",
    "\n",
    "#         # Optional activation functions\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):        \n",
    "#         # -1, 49, 768, --> -1, 49, 2560 \n",
    "#         x = self.relu(self.linear_3(self.relu(self.linear_2(self.relu(self.linear_1(x))))))    \n",
    "        \n",
    "#         x = self.projection_1(x)  # -1, 49, 2560, --> -1, 49, 2560\n",
    "        \n",
    "#         x = x.swapaxes(-2, -1)    # -1, 2560, 49\n",
    "#         x = self.fc4(x)           # -1, 2560, 49 --> -1, 2560, 75\n",
    "        \n",
    "#         x = x.swapaxes(-2, -1)    # -1, 75, 2560\n",
    "#         x = self.projection_1(x)\n",
    "        \n",
    "#         return x\n",
    "        \n",
    "\n",
    "# model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 32\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size_train, shuffle=True, num_workers=8)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3, eps=1e-9) \n",
    "normalize = transforms.Normalize(mean = 0, std = 1)\n",
    "\n",
    "# Number of epochs after which to decrease the learning rate\n",
    "step_size = 1\n",
    "\n",
    "# Factor by which to decrease the learning rate\n",
    "gamma = 0.1\n",
    "\n",
    "# Create StepLR scheduler\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime \n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"step_lr{datetime.now().strftime('%b%d_%H-%M-%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on epoch 0\n",
      "Loss: tensor(1.0002, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7540, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7239, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7025, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6794, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6827, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6659, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6534, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6924, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6804, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6506, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6456, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6303, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6520, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6434, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6440, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6570, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6346, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6387, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6706, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6909, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6488, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6228, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6705, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6279, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6331, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6211, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6389, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6191, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6406, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6277, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6594, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6373, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6514, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6327, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6594, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6260, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6128, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6553, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6350, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6415, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6413, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6484, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6626, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6290, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6115, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6217, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6260, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6214, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6522, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6455, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6237, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6563, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6590, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6465, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6513, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6369, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6439, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6282, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6286, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6484, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6280, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6267, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6528, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6322, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6480, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6094, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6413, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6577, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6234, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6150, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6211, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6364, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6336, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6131, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6412, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6359, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6653, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6603, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6193, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6303, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6473, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6234, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6468, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6574, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6652, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6351, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6448, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6372, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6315, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6588, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6512, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6040, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6466, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6214, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6238, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6496, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6382, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6515, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6255, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6247, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6447, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6399, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6324, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6298, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6462, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6361, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6262, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6733, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6287, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6174, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6206, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6553, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6474, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6586, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6109, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6345, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6724, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6424, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6538, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6315, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6107, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6193, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6269, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6314, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6274, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6502, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6463, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6428, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.5974, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6254, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6253, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6575, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6223, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6450, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6183, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6454, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6269, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6200, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6367, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6304, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6275, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6432, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6218, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6117, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6399, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6139, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6282, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6362, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6593, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6383, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6159, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6326, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6133, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6190, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6457, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6321, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6414, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6476, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6373, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6053, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6396, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6561, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6281, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6376, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6058, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6498, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6408, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.5860, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6611, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6332, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6493, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6344, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6879, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6534, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6212, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6414, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6218, grad_fn=<RsubBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.5999, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6354, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6473, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6316, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6314, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6495, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6250, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6164, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6010, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6430, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6293, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.5885, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6293, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6347, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6584, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6339, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6645, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6585, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6257, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6505, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6523, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6397, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6412, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6042, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6445, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6585, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6075, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6613, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6444, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6381, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6547, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6395, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6350, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6168, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6651, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6271, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6600, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6212, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6491, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6327, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6108, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6405, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6323, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6500, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6136, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6421, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6380, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6365, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6397, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6486, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6388, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.5967, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6494, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6370, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.5962, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6302, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6238, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6139, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6123, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6347, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6247, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6320, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6324, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6323, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6258, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6364, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6226, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6556, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6464, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6145, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6392, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6222, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6104, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6147, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6340, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6370, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6165, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6167, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6206, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6203, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6369, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6264, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6562, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6289, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6363, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6309, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6231, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6177, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6636, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6039, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6402, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6333, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6541, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6318, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6275, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6671, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6633, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6602, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6237, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6259, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6544, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6153, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6412, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6236, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6631, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6156, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6157, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6133, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6467, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6363, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6220, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6405, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6358, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6295, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6464, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6425, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6476, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6577, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6370, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6411, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6369, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6397, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6076, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6063, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6327, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6526, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6524, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6365, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6585, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6230, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6562, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6253, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6646, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6287, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6364, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6198, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6241, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6286, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6086, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6347, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6070, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6291, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6037, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6064, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6474, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6334, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6415, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6065, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6248, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6213, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6399, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6510, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6331, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6258, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6280, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6447, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6365, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6748, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6103, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6256, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6071, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6421, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6585, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.5993, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6546, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6475, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6182, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6117, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6486, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6210, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6361, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6101, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6316, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6521, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6140, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6474, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6632, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6444, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6349, grad_fn=<RsubBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.6123, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6368, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6325, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6317, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6379, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6526, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6287, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6523, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6532, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6588, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6418, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6278, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6298, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6091, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6212, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6467, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6365, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6320, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6480, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6173, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6472, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6297, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6548, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6505, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6215, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6456, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6201, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6523, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6061, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6659, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6375, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6054, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6260, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6377, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6377, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.5974, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6262, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6197, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6212, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6518, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6238, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6341, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6290, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6275, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6404, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6534, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6316, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6268, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6341, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6368, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6160, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6520, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6465, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6313, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6415, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6353, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6097, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6374, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6279, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6223, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6389, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6446, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6671, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6254, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6388, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6206, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6204, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6404, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6420, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6309, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6719, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6499, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6395, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6184, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6457, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6539, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6284, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6234, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6544, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6321, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6500, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.6404, grad_fn=<RsubBackward1>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m cosine_sim\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss)\n\u001b[1;32m     23\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss/train\u001b[39m\u001b[38;5;124m'\u001b[39m, loss, count)\n",
      "File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     adam(\n\u001b[1;32m    164\u001b[0m         params_with_grad,\n\u001b[1;32m    165\u001b[0m         grads,\n\u001b[1;32m    166\u001b[0m         exp_avgs,\n\u001b[1;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    169\u001b[0m         state_steps,\n\u001b[1;32m    170\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    171\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    172\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    173\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    174\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    175\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    176\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    177\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    178\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    179\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    180\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    181\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    182\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m func(params,\n\u001b[1;32m    312\u001b[0m      grads,\n\u001b[1;32m    313\u001b[0m      exp_avgs,\n\u001b[1;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    316\u001b[0m      state_steps,\n\u001b[1;32m    317\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    318\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    319\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    320\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    321\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    322\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    323\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    324\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    325\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    326\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    327\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/torch/optim/adam.py:385\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    384\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 385\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    388\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "count = 0 \n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs): \n",
    "    \n",
    "    print(f\"Working on epoch {epoch}\")\n",
    "    for iteration, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        input_ = batch[0]\n",
    "        gt_ = batch[1]\n",
    "          \n",
    "        output_ = model(input_)\n",
    "                \n",
    "        cosine_sim = F.cosine_similarity(output_, gt_).mean()\n",
    "        loss = 1 - cosine_sim\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Loss:\", loss)\n",
    "        writer.add_scalar('Loss/train', loss, count)\n",
    "        count += 1\n",
    "        \n",
    "        if count % 150 == 0: \n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/era/lib/python3.11/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|endoftext|> play and and how and how and why the the the the the the the the the the the']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(phi2_model_pretrained.generate(inputs_embeds=output_[0, :, :].unsqueeze(0), bos_token_id=tokenizer.bos_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>\\n\\n\\nQuestion 1: If a person is feeling anxious, they may experience physical symptoms such as']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(phi2_model_pretrained.generate(inputs_embeds=gt_[0, :, :].unsqueeze(0), bos_token_id=tokenizer.bos_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
